We utilized two datasets that already contains images of eyes and that have been labeled like [MRL Eye Dataset](https://data.mendeley.com/datasets/vy4n28334m/1) and [CEW Dataset](https://www.kaggle.com/datasets/imadeddinedjerarda/mrl-eye-dataset). The Mendeley eye_tracker_data dataset provided 7,500 evenly distributed eye images labeled in the five directional categories we needed (left, right, up, down, and straight), captured using standard camera equipment. Additionally, the MRL Eye Dataset offered over 300 MB of images specifically focused on distinguishing between open and closed eyes, providing blinking detection capability. Together, these datasets allowed us to focus on building and training our neural network rather without complex data preprocessing, while still ensuring sufficient diversity in eye shapes, lighting conditions, and head positions to create a robust model. We combined the two datasets to have a more comprehensive dataset that included the following: left, right, up, down, straight, and closed. This allowed us to train a model that could accurately classify eye movements and detect blinks in real-time.

blah blah balh we train model with regular CNN and SWIN transfomers.

After generating our models, we decided to utilize Gradio as the web interface to process the model based on your device's webcam feed and classify the eye movements to guide the robot. By integrating Gradio with our trained model, we could provide an interactive experience where users could see the model's predictions on their eye movements through their webcam feed. In order to detect a user's eyes within the web stream, we utilized an existing face detection model from dlib that applied landmarks to a user's face. We could then identify where a user's eyes were by focusing on certain landmarks. Francisco applied this pretrained model to a python script to detect a user's eyes and isolate them into a smaller window screen which also had a few linear transformations applied to it to uniformly orient all images. This python file was developed in the Summer of 2024 by Francisco as part of his SURP project for Eyes In Motion.

For the gradio interface, we wanted a simple way for users to be able to control the robot themselves. Calling our application Eye Gaze Direction Detection, this interface as a web application that leveraged computer vision and deep learning for real-time eye tracking. Our system captured video frames through a webcam and used the dlib library to detect facial landmarks, effectively isolating eye regions for processing. We then implemented the two different deep learning models that we created and stored as pickle files, a custom CNN and a Swin Transformer, to predict gaze direction.  We classified it into multiple categories including left, right, up, down, straight, and closed. The frames would start processing based on a start and stop button that triggered a javascript function to continuously predecit each frame. Our frame image processing techniques incorporated frame rotation to normalize eye orientation, region cropping and resizing, and fixed bounding box calculation for consistency. We solved the refresh challenge common in web-based video of getting continuous real time prediciton by inserting a javascript listener that triggered a function to update the video feed every 1000 milliseconds. This allowed us to have a real time prediction of the user's eye movements and blink detection. The model's prediction and confidence of the prediciton were displayed on the Gradio interface, providing users with immediate feedback on their gaze direction.

THe Gradio interface displays a clean, organized two-column layout with clear user instructions, a primary video display showing processed eye regions with overlay information, and real-time gaze predictions with confidence scores. We designed it so users could easily control the system with a simple start/stop camera button, while status indicators showed the current system state. Additonally, we included robust error handling and a comprehensive debug system using debug_print statements. 

